# -*- coding: utf-8 -*-
"""Entrega1_ProcesamientodeDatos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yjJ61JlH5oo8IC3eZfss7wRZ4DpcmebZ

# Momento de Retroalimentación: Módulo 1 Técnicas de procesamiento de datos para el análisis estadístico y para la construcción de modelos (Portafolio Análisis)

Comenzamos por cargar el dataset que vamos a preprocesar.
"""

from google.colab import drive

drive.mount("/content/gdrive")
!pwd  # show current path

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/gdrive/MyDrive/Colab Notebooks"
!ls  # show current directory

"""Aqui lo cargamos y se lo asignamos  a la variable llamada data. Debido a que esta delimitado por ";" en el archivo csv, especificamos eso dentro del codigo para poder cargarlo de forma correcta y que tenga formato."""

import pandas as pd
data = pd.read_csv("bank-full.csv", delimiter=';')

data.head()

"""Buscamos obtener los tipos de variables, del dataset y separarlos en numericas o categoricas, esto para poder plantear de mejor forma lo que se busca realizar a futuro con el datase, que es predecir si los clientes se suscriben o no, en base a las llamadas que reciben."""

data.info()

"""Las variables categoricas y numericas detectadas son las siguientes:

Las variables numéricas son aquellas que tienen un tipo de dato int64 y representan cantidades o medidas que se pueden ordenar y operar matemáticamente.

- age: Edad del cliente (int64)
- balance: Saldo promedio anual en euros (int64)
- day: Día del mes en que se realizó el último contacto (int64)
- duration: Duración del último contacto en segundos (int64)
- campaign: Número de contactos realizados durante esta campaña (int64)
- pdays: Número de días desde el último contacto en una campaña anterior (int64)
- previous: Número de contactos realizados antes de esta campaña (int64)
- Variables Categóricas

Las variables categóricas son aquellas que tienen un tipo de dato object y - representan categorías o etiquetas que no tienen un orden inherente.

- job: Tipo de trabajo (object)
- marital: Estado civil (object)
- education: Nivel educativo (object)
- default: Si tiene crédito en incumplimiento (sí/no) (object)
- housing: Si tiene préstamo hipotecario (sí/no) (object)
- loan: Si tiene préstamo personal (sí/no) (object)
- contact: Tipo de contacto (object)
- month: Mes del último contacto (object)
- poutcome: Resultado de la campaña anterior (object)
- y: Si el cliente se suscribió a un depósito a plazo fijo (sí/no) (object) - (variable objetivo)

Obtendremos graficas de caja para lograr identificar outliers, que son valores que estan muy alejados de la mayoria de los datos, lo que nos puede traer problemas mas adelante
al momento de realizar el analisis o lo modelos estadisticos que busco implementar, por eso, vamos a eliminarlos. Esto tytrabajando sobre nuestras variables numericas.
"""

import matplotlib.pyplot as plt
import seaborn as sns

numeric_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

plt.figure(figsize=(15, 10))
for i, column in enumerate(numeric_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=data[column])
    plt.title(f'Boxplot of {column}')

plt.tight_layout()
plt.show()

"""Para poder eliminar los Outliers, investigue de un tema llamado Rango Intercuartílico (IQR), que basicamente es una medida que ayuda a entender la dispersión de los datos entre dos distintos cuatiles Q1 Y Q3:
- Q1 = Es el primer cuartil, de el valor por debajo del 25%.
- Q3 = Es el tercer cuartil, de el valor que este por debajo del 75%.

Siguiendo la siguiente formula:
IQR = Q3 - Q1

 El resultado de esto nos da una rango central donde se encuentren los datos de alredor del 50%, los demas datos que esten fuera de este rango podrian afectar negativamente modelos que posterioremnte se implementaran, ya que son valores o muy grandes o muy pequeños que podrian causar un ruido que se puede evitar.

- Límite Inferior: Se define como Q1 - 1.5 * IQR. Cualquier valor por debajo de este límite se considera un outlier inferior.
- Límite Superior: Se define como Q3 + 1.5 * IQR. Cualquier valor por encima de este límite se considera un outlier superior.
"""

def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Eliminamos los outliers para cada columna numérica donde se detectaron
columns_with_outliers = ['balance', 'duration', 'pdays', 'previous']

for column in columns_with_outliers:
    data = remove_outliers(data, column)

data.shape

"""Despues, comprobamos si es que existen registros vacios o duplicados en el dataset, para poder eliminarlos o tratarlos si es necesario."""

missing_values = data.isnull().sum()
duplicate_records = data.duplicated().sum()
missing_values, duplicate_records

import missingno as msno
msno.bar(data)

"""Procederemos a eliminar columnas que determino que no tienen reelevancia en la problematica son las siguientes columnas:
- day
- month
- pdays
- previous
- poutcome
- contact


"""

columns_to_delete = ['day', 'month', 'pdays', 'previous', 'poutcome', 'contact']
data1 = data.drop(columns=columns_to_delete)

data1.head()

data1.columns

"""Deespues de esto, dejamos solo las columnas mas reelevantes para el analisis y la prediccion de nuestro modelo.

Ahora, busco unificar o disminuir las categorias disponibles en algunas columnas, como en la de los empleos para hacer mas sencilla la clasificacion de las variables y poder hacer el one hot encoding de mejor forma, ya que el trabajo si es algo que puede ser reelevante para mi analisis y el reducir las categorias dentro de esta columna hara mas sencillo la manipulacion de estos datos. También hice el analisis en la columna de Education, solo que en esta solo hay 4, mientras que en job existen 12 distintas.
"""

unique_jobs = data1['job'].unique()
unique_education = data1['education'].unique()

print("Unique values in 'job':")
print(unique_jobs)

print("\nUnique values in 'education':")
print(unique_education)

"""Para simplificar, unifique en un diccionario para disminuir de 12 a 7 solamente las variables dentro de la categoria jobs."""

# Crear un diccionario con las nuevas agrupaciones
job_map = {
    'management': 'Management',
    'admin.': 'Management',
    'entrepreneur': 'Management',
    'technician': 'Technical',
    'blue-collar': 'Technical',
    'services': 'Technical',
    'self-employed': 'Technical',
    'housemaid': 'Unskilled',
    'unemployed': 'Unemployed',
    'retired': 'Unemployed',
    'student': 'Student',
    'unknown': 'Other'
}

data1['job'] = data1['job'].replace(job_map)

print(data['job'].unique())

data1.head()

column_names = data1.columns
print(column_names)

"""Ahora aplicare la Tecnica de One Hot Encoding para convertir las variables categoricas a numericas y poder trabajar mas facilmente con ellas. Esto con el fin de que sea mas sencillo trabajar con los datos, que sol es necesario manejar como tipo booleano, "true o false" o "0/1", esto para que al momento de implementar mi modelo de prediccion, sea mas sencillo el alimentarlo con este tipo de datos."""

data_encoded = pd.get_dummies(data1, columns=['job', 'marital', 'education', 'default', 'housing', 'loan'], drop_first=True)

data_encoded.head()

"""Posteriormente usare la funcion de standarscaler de la libreria de sklearn para hacer el escalamiento o normalizacion de los datos, esto para ue tenga una media de 0 y una desviacion estandar de 1 para que no tenga problemas el modelo al momento de implementarlo."""

import pandas as pd
from sklearn.preprocessing import StandardScaler

numeric_columns = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']

scaler = StandardScaler()

data_encoded[numeric_columns] = scaler.fit_transform(data[numeric_columns])

# Verificar las primeras filas para confirmar la transformación
print(data.head())

data_encoded.to_csv('bank_preprocessed.csv', index=False)

