# -*- coding: utf-8 -*-
"""Entrega1_SinFramework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q-BFG0FOicRsr33IpAhz8gbn9o2m-TPW

Momento de Retroalimentación: Módulo 2 Implementación de una técnica de aprendizaje máquina sin el uso de un framework. (Portafolio Implementación).

Link al documento del reporte escrito: [Reporte preliminar](https://docs.google.com/document/d/1_4Veqd4stUotideEh4BgP8AjQpYAAqiO/edit?usp=sharing&ouid=114849935084064003871&rtpof=true&sd=true)
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Inicializacion de parametros con valores pequeños y aleatorios
def initialize_parameters(n_features):
    np.random.seed(0)
    weights = np.random.randn(n_features) * 0.01  #Se multiplica para obtener valores pequeños
    bias = 0.0
    return weights, bias

#Funcion de activacion sigmoide
def sigmoid_activation(z):
    return 1 / (1 + np.exp(-z))

#Funcion para calcular el costo (cross entropy)
def calculate_cost(y_true, y_pred):
    m = len(y_true)
    cost = -(1/m) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))
    return cost

#Algoritmo de optimizacion mediante Gradiente Descendente
def gradient_optimization(X, y, weights, bias, learning_rate, num_iterations):
    m = X.shape[0]
    loss_history = []
    for i in range(num_iterations):
        #Prediccion
        z = np.dot(X, weights) + bias
        y_pred = sigmoid_activation(z)
        #Calculo del gradiente
        dw = (1/m) * np.dot(X.T, (y_pred - y))
        db = (1/m) * np.sum(y_pred - y)
        #Actualizacion de los parametros
        weights -= learning_rate * dw
        bias -= learning_rate * db
        #aAlmacenar la funcion de costo
        cost = calculate_cost(y, y_pred)
        loss_history.append(cost)
        #Imprimir el costo cada 100 iteraciones
        if i % 100 == 0:
            print(f"Perdida/Costo {i}: {cost}")
    return weights, bias, loss_history

#Funcion para hacer predicciones
def make_predictions(X, weights, bias, threshold=0.5):
    z = np.dot(X, weights) + bias
    y_pred = sigmoid_activation(z)
    return [1 if i > threshold else 0 for i in y_pred]

#Funcion para dividir los datos en entrenamiento y prueba manualmente
def manual_train_test_split(X, y, test_size=0.3, random_state=None):
    if random_state is not None:
        np.random.seed(random_state)
    indices = np.arange(X.shape[0])
    np.random.shuffle(indices)
    test_size = int(len(indices) * test_size)
    test_indices = indices[:test_size]
    train_indices = indices[test_size:]
    X_train, X_test = X[train_indices], X[test_indices]
    y_train, y_test = y[train_indices], y[test_indices]
    return X_train, X_test, y_train, y_test

#Cargar el dataset preprocesado
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/bank_preprocessed.csv')

#Preparar las variables dependientes e independientes del dataset
X = df.drop(columns=['y']).values
y = np.where(df['y'] == 'yes', 1, 0)  # Convertir 'yes' a 1 y 'no' a 0

#Asegurar que todos los datos esten en formato numérico int/float
X = X.astype(np.float64)

#Dividir los datos en conjuntos de entrenamiento "Train"(70%) y prueba "Test" (30%)
X_train, X_test, y_train, y_test = manual_train_test_split(X, y, test_size=0.3, random_state=0)

#Configurar los hiperparametros
learning_rate = 0.001
num_iterations = 1000

#Inicializar los pesos y el bias
n_features = X_train.shape[1]
weights, bias = initialize_parameters(n_features)

#Entrenar el modelo en el conjunto de entrenamiento "Train"
weights, bias, loss_history = gradient_optimization(X_train, y_train, weights, bias, learning_rate, num_iterations)

#Graficar la función de perdida
plt.figure(figsize=(10, 6))
plt.plot(range(len(loss_history)), loss_history, color='blue')
plt.title("Función de Perdida")
plt.xlabel("Iteraciones")
plt.ylabel("Perdida")
plt.grid(True)
plt.show()

#Realizar predicciones en el conjunto de prueba "Test"
y_pred_test = make_predictions(X_test, weights, bias)

#Calcular la precisión del modelo
accuracy_test = np.mean(y_pred_test == y_test)
print(f"Precisión en los datos de prueba: {accuracy_test}")

